{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase 8 - Knn\n",
    "\n",
    "**KNN: K-Nearest Neighbors o K vecinos más cercanos**\n",
    "\n",
    "EL METODO SIRVE COMO PARA HACER PREDDCIONES COMO REGRESIONES, EL VALOR DE K DEPENDE DE LA EXPERIENCIA\n",
    "\n",
    "Es un método de clasificación no paramétrico; es decir, no requiere asumir ninguna distribución para la variable aleatoria X=(X1,X2,...,Xp). Este método no requiere estimar las probabilidades desconocidas πg de que un elemento seleccionado al azar provenga de la población g.\n",
    "\n",
    "La idea es buscar, para la nueva observación que queremos clasificar, sus K vecinos más cercanos, es decir, las K observaciones más cercanas respecto a una medida de distancia.\n",
    "\n",
    "El algoritmo es el siguiente:\n",
    "\n",
    "1. Definimos una medida de distancia adecuada para las observaciones.\n",
    "2. Calculamos la distancia entre la nueva observación x0 que queremos clasificar, y las observaciones que tenemos en nuestra matriz de datos.\n",
    "3. Seleccionamos las K observaciones más cercanas a x0, y miramos a qué grupo pertenecen.\n",
    "4. Clasificamos x0 en la población a la que pertenece una mayor proporción de sus K vecinos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = [12, 12]\n",
    "np.random.seed(42)\n",
    "\n",
    "# Vamos a usar este dataset para probar KNN en clasificación y en regresión\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#F1 SOLO SIRVE PARA CLASIFICACIONES, (OBSERVAR QUE TAN BUEN CLASIFICADOR FUE)\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver como vamos a usar el algoritmo KNN en scikit-learn.\n",
    "El algoritmo KNN se puede usar tanto en problemas de clasificación\n",
    "(con el estimador KNeighborsClassifier) como en problemas de regresión(con el estimador KNeighborsRegressor)\n",
    "\n",
    "**KMEDIAS ES UN METODO NO SUPERVISADO, ES DECIR QUE NO CONOCES LAS CLASIFICACIONES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>año</th>\n",
       "      <th>ratings</th>\n",
       "      <th>genero</th>\n",
       "      <th>ventas</th>\n",
       "      <th>presupuesto</th>\n",
       "      <th>secuela</th>\n",
       "      <th>vistas_youtube</th>\n",
       "      <th>positivos_youtube</th>\n",
       "      <th>negativos_youtube</th>\n",
       "      <th>comentarios</th>\n",
       "      <th>seguidores_agregados</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014</td>\n",
       "      <td>6.3</td>\n",
       "      <td>8</td>\n",
       "      <td>9130</td>\n",
       "      <td>4000000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3280543</td>\n",
       "      <td>4632</td>\n",
       "      <td>425</td>\n",
       "      <td>636</td>\n",
       "      <td>1120000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014</td>\n",
       "      <td>7.1</td>\n",
       "      <td>1</td>\n",
       "      <td>192000000</td>\n",
       "      <td>50000000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>583289</td>\n",
       "      <td>3465</td>\n",
       "      <td>61</td>\n",
       "      <td>186</td>\n",
       "      <td>12350000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014</td>\n",
       "      <td>6.2</td>\n",
       "      <td>1</td>\n",
       "      <td>30700000</td>\n",
       "      <td>28000000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>304861</td>\n",
       "      <td>328</td>\n",
       "      <td>34</td>\n",
       "      <td>47</td>\n",
       "      <td>483000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014</td>\n",
       "      <td>6.3</td>\n",
       "      <td>1</td>\n",
       "      <td>106000000</td>\n",
       "      <td>110000000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>452917</td>\n",
       "      <td>2429</td>\n",
       "      <td>132</td>\n",
       "      <td>590</td>\n",
       "      <td>568000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014</td>\n",
       "      <td>4.7</td>\n",
       "      <td>8</td>\n",
       "      <td>17300000</td>\n",
       "      <td>3500000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3145573</td>\n",
       "      <td>12163</td>\n",
       "      <td>610</td>\n",
       "      <td>1082</td>\n",
       "      <td>1923800.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    año  ratings  genero     ventas  presupuesto  secuela  vistas_youtube  \\\n",
       "0  2014      6.3       8       9130    4000000.0        1         3280543   \n",
       "1  2014      7.1       1  192000000   50000000.0        2          583289   \n",
       "2  2014      6.2       1   30700000   28000000.0        1          304861   \n",
       "3  2014      6.3       1  106000000  110000000.0        2          452917   \n",
       "4  2014      4.7       8   17300000    3500000.0        2         3145573   \n",
       "\n",
       "   positivos_youtube  negativos_youtube  comentarios  seguidores_agregados  \n",
       "0               4632                425          636             1120000.0  \n",
       "1               3465                 61          186            12350000.0  \n",
       "2                328                 34           47              483000.0  \n",
       "3               2429                132          590              568000.0  \n",
       "4              12163                610         1082             1923800.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para este ejemplo vamos a usar el dataset CSM (Conventional and Social Media Movies) \n",
    "# que contiene información de la popularidad en redes sociales de distintas películas \n",
    "# así como las ventas en taquilla.\n",
    "pelis = pd.read_csv(\"datos_peliculas.csv\")\n",
    "#SE QUITA EL TITULO PORQUE NO APORTA INFORMACION IMPORTANTE\n",
    "pelis = pelis.drop(\"pelicula\", axis=1) \n",
    "pelis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SE ELIGE COMO LA VARIABLE OBJETIVO EL GENERO\n",
    "variable_objetivo_clasificacion = \"genero\"\n",
    "#SE QUITA LA COLUMNA DE LA VARIABLE OBJETIVO PARA OBTENER LAS VARIABLES INDEPENDIENTES\n",
    "variables_independientes_clasificacion = pelis.drop(variable_objetivo_clasificacion, axis=1).columns\n",
    "#SE HACE LA PARTICION DE ENTRENAMIENTO Y PRUEBA \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    pelis[variables_independientes_clasificacion],\n",
    "    pelis[variable_objetivo_clasificacion], test_size=0.20, random_state=2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los parámetros más importantes a la hora de usar KNeighborsClasifier son:\n",
    "- **n_neighbors**: El valor de K, es decir el número de vecinos que considerar a la hora de asignar una clase.\n",
    "- **weights**: A la hora de votar, que importancia dar a los vecinos. Si elegimos auto asigna la misma importancia a todos los vecinos. Si elegimos distance asigna importancia a los vecinos en función de la distancia de los vecinos al punto a clasificar\n",
    "- **metric**: La métrica a la hora de medir la distancia entre los puntos. Si se usa distancia de Minkowsky se puede elegir p con el parámetro p, que por defecto es 2 (lo que computa la distancia euclidiana).\n",
    "\n",
    "En este caso en particular sabemos que valor elegir de K, ya que podemos asumir  que el número de categorías del dataset es el total de categorías de películas del dataset de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utilizando Weigths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score para K clasificador utilizando peso uniforme: \n",
      "0.40425531914893614\n"
     ]
    }
   ],
   "source": [
    "#COMO PUNTO DE PARTIDA SE TOMA LA LONGITUD DE LAS CATEGORIAS COMO PRIMER VALOR DE K\n",
    "k_categorias = len(y_train.unique())\n",
    "#SE APLICA EL CLASIFICADOR DANDO EL PESO UNIFORME\n",
    "clasificador_knn = KNeighborsClassifier(n_neighbors=100, weights=\"uniform\")\n",
    "#SE LLENA CON LOS VALORES DE ENTRENAMIENTO\n",
    "clasificador_knn.fit(X_train, y_train)\n",
    "#SE HACEN LAS PREDICCIONES Y SE OBTINENE EL F1 SCORE\n",
    "preds = clasificador_knn.predict(X_test)\n",
    "print(\"f1 score para K clasificador utilizando peso uniforme: \")\n",
    "print(f1_score(y_test, preds, average=\"micro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utilizando distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score para K clasificador utilizando peso de distancia: \n",
      "0.3191489361702128\n"
     ]
    }
   ],
   "source": [
    "# Si ahora entrenamos el estimador con el argumento de pesos weights=\"distance\", \n",
    "# vemos que funciona de forma ligeramente mejor.\n",
    "clasificador_knn = KNeighborsClassifier(n_neighbors=100, weights=\"distance\")\n",
    "clasificador_knn.fit(X_train, y_train)\n",
    "preds = clasificador_knn.predict(X_test)\n",
    "print(\"f1 score para K clasificador utilizando peso de distancia: \")\n",
    "print(f1_score(y_test, preds, average=\"micro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NO PORQUE AUMENTES EL NUMERO DE VECINOS LA CLASIFICACION MEJORA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probando para diferentes valores de **K**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 0.23404255319148937, 0.23404255319148937)\n",
      "(3, 0.2978723404255319, 0.2553191489361702)\n",
      "(5, 0.2765957446808511, 0.23404255319148937)\n",
      "(7, 0.23404255319148937, 0.21276595744680848)\n",
      "(9, 0.2553191489361702, 0.2553191489361702)\n",
      "(11, 0.2765957446808511, 0.2553191489361702)\n",
      "(13, 0.2978723404255319, 0.3191489361702128)\n",
      "(15, 0.23404255319148937, 0.2978723404255319)\n",
      "(17, 0.23404255319148937, 0.2765957446808511)\n",
      "(19, 0.23404255319148937, 0.2553191489361702)\n",
      "(21, 0.2765957446808511, 0.2553191489361702)\n",
      "(23, 0.2553191489361702, 0.2765957446808511)\n",
      "(25, 0.23404255319148937, 0.2765957446808511)\n",
      "(27, 0.2765957446808511, 0.2978723404255319)\n",
      "(29, 0.2765957446808511, 0.2978723404255319)\n",
      "(31, 0.2765957446808511, 0.2765957446808511)\n",
      "(33, 0.2978723404255319, 0.2978723404255319)\n",
      "(35, 0.3191489361702128, 0.2978723404255319)\n",
      "(37, 0.3404255319148936, 0.2978723404255319)\n",
      "(39, 0.3191489361702128, 0.2765957446808511)\n",
      "(41, 0.3404255319148936, 0.3191489361702128)\n",
      "(43, 0.3404255319148936, 0.3191489361702128)\n",
      "(45, 0.3191489361702128, 0.2978723404255319)\n",
      "(47, 0.3191489361702128, 0.3191489361702128)\n",
      "(49, 0.3191489361702128, 0.3191489361702128)\n"
     ]
    }
   ],
   "source": [
    "#SE UTILIZA UN CICLO PARA PROBAR LOS VALORES DE K\n",
    "for k in range(1,51,2):\n",
    "    #PARA UNIFORME\n",
    "    clasificador_knn_unif = KNeighborsClassifier(n_neighbors=k, weights=\"uniform\")\n",
    "    clasificador_knn_unif.fit(X_train, y_train)\n",
    "    preds_unif = clasificador_knn_unif.predict(X_test)\n",
    "    \n",
    "    #PARA DISTANCIAS\n",
    "    clasificador_knn_distance = KNeighborsClassifier(n_neighbors=k, weights=\"distance\")\n",
    "    clasificador_knn_distance.fit(X_train, y_train)\n",
    "    preds_distance = clasificador_knn_distance.predict(X_test)\n",
    "    \n",
    "    #SE IMPRIMEN LOS VALORES\n",
    "    print(f'{(k,f1_score(y_test, preds_unif, average=\"micro\"),f1_score(y_test, preds_distance, average=\"micro\"))}')\n",
    "\n",
    "#Podemos usar el método kneighbors para devolver los k vecinos de un punto en concreto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "año                         2014.0\n",
      "ratings                        5.2\n",
      "ventas                  85900000.0\n",
      "presupuesto             65000000.0\n",
      "secuela                        1.0\n",
      "vistas_youtube            930006.0\n",
      "positivos_youtube           5150.0\n",
      "negativos_youtube            707.0\n",
      "comentarios                 1484.0\n",
      "seguidores_agregados     5130800.0\n",
      "Name: 13, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#SE IMPRIME EL PRIMER RENGLON DE LA TABLA ORIGINAL\n",
    "print(X_test.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4622670.47679153]] [[129]]\n",
      "     año  ratings    ventas  presupuesto  secuela  vistas_youtube  \\\n",
      "40  2014      7.6  85700000   68000000.0        1         2276605   \n",
      "\n",
      "    positivos_youtube  negativos_youtube  comentarios  seguidores_agregados  \n",
      "40               3946                331         1286             1888000.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\felip\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#AQUI SE OBTIENE EL VECINO MAS CERCANO DEL PRIMER RENGLON DE X_test\n",
    "distancia, indice = clasificador_knn.kneighbors([X_test.iloc[0]], n_neighbors=1)\n",
    "print(distancia,indice)\n",
    "\n",
    "#EN DISTANCIA SE GUARDA LA DISTANCIA Y EL INDICE LLEVA EL INDICE DEL VECINO MAS CERCANO\n",
    "print(X_train.iloc[indice[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN para problemas de regresión\n",
    "\n",
    "Vamos a utilizar ahora el algoritmo KNN para un problema de regresión,KNN funciona igual para hacer regresiones, simplemente que en vez de una votación donde la clase más común entre los vecinos más próximos es la elegida, se hace una interpolación de los valores de la variable numérica objetivo de los vecinos.\n",
    "\n",
    "En concreto vamos a estimar las ventas de entradas en taquilla de una película en función de su popularidad online y presupuesto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43959789.048082694\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#SI DIVIDE EL DATA FRAME PARA EL OBJETIVO Y LAS VARIABLES INDEPENDIENTES\n",
    "variable_objetivo_regresion = \"ventas\"\n",
    "variables_independientes_regresion = pelis.drop(variable_objetivo_regresion, axis=1).columns\n",
    "\n",
    "#SE DIVIDE EN ENTRENAMIENTO Y PRUEBA\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    pelis[variables_independientes_regresion],\n",
    "    pelis[variable_objetivo_regresion], test_size=0.20)\n",
    "\n",
    "# Usamos la implementación en sklearn KNeighborsRegressor para problemas de regresión.\n",
    "# Tiene los mismos hiperparámetros que NeighborsClassifier.\n",
    "\n",
    "#SE CREA EL ESTIMADOR DANDO EL NUMERO DE VECINOS Y EL PARAMETRO DE PESO\n",
    "regresor_knn = KNeighborsRegressor(n_neighbors=10, weights=\"distance\")\n",
    "\n",
    "#SE LLENA EL ESTIMADOR CON LOS DATOS DE ENTRENAMIENTO\n",
    "regresor_knn.fit(X_train, y_train)\n",
    "\n",
    "#SE OBTIENEN LAS PREDICCIONES\n",
    "preds = regresor_knn.predict(X_test)\n",
    "\n",
    "#SE IMPRIME EL ERROR ENTRE LAS VARIABLES DE PRUEBA Y LAS PREDICCIONES\n",
    "print(np.sqrt(np.abs(mean_squared_error(y_test, preds))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La puntuación F1 de KNN para clasificacion en este dataset es 0.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\felip\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=20.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#PARA TECNICAS DE CLASIFICICACION\n",
    "#SE HACE LA PRUEBA DE VALIDACION CRUZADA\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "error_validacion_cruzada_clasificacion = np.sqrt(np.abs(\n",
    "    cross_val_score(KNeighborsClassifier(\n",
    "                n_neighbors=39, \n",
    "                weights=\"distance\"), \n",
    "                X=pelis[variables_independientes_clasificacion],\n",
    "                y=pelis[variable_objetivo_clasificacion], \n",
    "                scoring=\"f1_micro\",\n",
    "                cv = 20\n",
    "        ).mean()\n",
    "      )\n",
    ")\n",
    "print(\"La puntuación F1 de KNN para clasificacion en este dataset es {:.2f}\".format(\n",
    "    error_validacion_cruzada_clasificacion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "83b617c66aa9ed92efeb8629453f4b5eb61ab11a86f5ad7ba589d02897e9d812"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
